{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author detection or author attribution is an important field in NLP that enables us\n",
    "to verify the authorship of papers or novels and allows us to identify anonymous\n",
    "authors.  \n",
    "\n",
    "\n",
    "In this first try to merge and understand what people have done at the works below. Also this notebook is designed to be an answer for the following Kaggle challenge\n",
    "\n",
    "\n",
    "* http://cs224d.stanford.edu/reports/ZhouWang.pdf\n",
    "* http://cs224d.stanford.edu/reports/MackeStephen.pdf\n",
    "* http://cs224d.stanford.edu/reports/RhodesDylan.pdf\n",
    "* http://cs224d.stanford.edu/reports/YaoLeon.pdf\n",
    "* https://www.kaggle.com/christopher22/stylometry-identify-authors-by-sentence-structure#\n",
    "* https://www.kaggle.com/drissaitlabsir27/author-identification-spacy-rnn#\n",
    "\n",
    "* https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author#\n",
    "* https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines#\n",
    "* https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31#\n",
    "* https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras#\n",
    "* https://www.kaggle.com/snapcrack/all-the-news/version/3/kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET EXPLORATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/author_identification/train.csv')\n",
    "# train['text'] = train.content\n",
    "# train = train[['id','text', 'author']]\n",
    "test = pd.read_csv('../Data/author_identification/test.csv')\n",
    "\n",
    "# sample = pd.read_csv('../Data/author_identification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id19764</td>\n",
       "      <td>Herbert West needed fresh bodies because his l...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id11411</td>\n",
       "      <td>Now the net work was not permanently fastened ...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id08075</td>\n",
       "      <td>It was not that the sounds were hideous, for t...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id00764</td>\n",
       "      <td>I was rich and young, and had a guardian appoi...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id01577</td>\n",
       "      <td>Wilbur's growth was indeed phenomenal, for wit...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id19764  Herbert West needed fresh bodies because his l...    HPL\n",
       "1  id11411  Now the net work was not permanently fastened ...    EAP\n",
       "2  id08075  It was not that the sounds were hideous, for t...    HPL\n",
       "3  id00764  I was rich and young, and had a guardian appoi...    MWS\n",
       "4  id01577  Wilbur's growth was indeed phenomenal, for wit...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.author.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further it is important that we split the data into training and validation sets. We can do it using train_test_split from the model_selection module of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see first the number of training instance and validation instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12764\n",
      "1419\n"
     ]
    }
   ],
   "source": [
    "print(len(xtrain))\n",
    "print(len(xvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f17516194e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD9CAYAAACrxZCnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1751d14d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style()\n",
    "sns.barplot(x=['Edgar Allen Poe', 'Mary Wollstonecraft Shelley', 'H.P. Lovecraft'], y = train.author.value_counts())\n",
    "# train.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edgar Allen Poe is the most frequently occuring author in the training dataset with 7900 occurances. Then it's Mary Wollstonecraft Shelley followed by H.P. Lovecraft with 6044 and 5635 occurances respectively. While there are more occurances of EAP than HPL and MWS there is still a good amount of data labeled with HPL and MWS so the small data imbalance shouldn't impact learning too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO See: https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create the function to find the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - TF - IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Always start with these features. They work (almost) everytime!\n",
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=0,\n",
    "            stop_words = 'english')\n",
    "\n",
    "# Fitting TF-IDF to both training and test sets (semi-supervised learning)\n",
    "tfv.fit(xtrain)\n",
    "xtrain_tfv =  tfv.transform(xtrain) \n",
    "xvalid_tfv = tfv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.647 \n",
      "Accuracy : 0.791\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on TFIDF\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "y_pred = np.array([f.argmax() for f in predictions])\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print(\"Accuracy : %0.3f\" %accuracy_score(yvalid, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 TF  + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "\n",
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "ctv.fit(list(xtrain))\n",
    "xtrain_ctv =  ctv.transform(xtrain) \n",
    "xvalid_ctv = ctv.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.539 \n",
      "Accuracy : 0.779\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Logistic Regression on Counts\n",
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "y_pred = np.array([f.argmax() for f in predictions])\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print(\"Accuracy : %0.3f\" %accuracy_score(yvalid, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TF - IDF + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.607 \n",
      "Accuracy : 0.803\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple Naive Bayes on TFIDF\n",
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_tfv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv)\n",
    "y_pred = np.array([f.argmax() for f in predictions])\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print(\"Accuracy : %0.3f\" %accuracy_score(yvalid, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF  + Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.468 \n",
      "Accuracy : 0.820\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(xtrain_ctv, ytrain)\n",
    "predictions = clf.predict_proba(xvalid_ctv)\n",
    "y_pred = np.array([f.argmax() for f in predictions])\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print(\"Accuracy : %0.3f\" %accuracy_score(yvalid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF  + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.768 \n",
      "Accuracy : 0.674\n"
     ]
    }
   ],
   "source": [
    "# Fitting a simple xgboost on tf-idf\n",
    "clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "clf.fit(xtrain_tfv.tocsc(), ytrain)\n",
    "predictions = clf.predict_proba(xvalid_tfv.tocsc())\n",
    "y_pred = np.array([f.argmax() for f in predictions])\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\n",
    "print(\"Accuracy : %0.3f\" %accuracy_score(yvalid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID Parameter Search for Models Till Now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's choose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics, pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize SVD\n",
    "svd = TruncatedSVD()\n",
    "    \n",
    "# Initialize the standard scaler \n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "# We will use logistic regression here..\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('lr', lr_model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'svd__n_components' : [120, 180],\n",
    "              'lr__C': [0.1, 1.0, 10], \n",
    "              'lr__penalty': ['l1', 'l2']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV] lr__C=0.1, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.8034970852047779, total=  17.5s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=120, score=-0.7988313008171505, total=  18.1s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=120 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   18.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=-0.808433065371859, total=  18.5s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=120, score=-0.8026936777082284, total=  19.2s\n",
      "[CV] lr__C=1.0, lr__penalty=l1, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.7567071388961919, total=  26.1s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=-0.7628331780322343, total=  27.1s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=120 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l2, svd__n_components=180, score=-0.7563438043568467, total=  28.7s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=0.1, lr__penalty=l1, svd__n_components=180, score=-0.7630660929768909, total=  28.8s\n",
      "[CV] lr__C=1.0, lr__penalty=l2, svd__n_components=180 ................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=-0.7897581985987758, total=  19.7s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   37.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=120, score=-0.7986410582634387, total=  21.3s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=120 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.7959856313704179, total=  17.1s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=120, score=-0.7948646398587027, total=  17.1s\n",
      "[CV] lr__C=10, lr__penalty=l1, svd__n_components=180 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  24 | elapsed:   45.0s remaining:   45.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=-0.7632832586895966, total=  33.3s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l1, svd__n_components=180, score=-0.7582143659514337, total=  35.1s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=120 .................\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=-0.7949280992662919, total=  16.7s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  24 | elapsed:   54.7s remaining:   32.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.7553176618661451, total=  25.9s\n",
      "[CV] lr__C=10, lr__penalty=l2, svd__n_components=180 .................\n",
      "[CV]  lr__C=1.0, lr__penalty=l2, svd__n_components=180, score=-0.7646054327096236, total=  28.2s\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=120, score=-0.7968529782606011, total=  19.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  24 | elapsed:  1.0min remaining:   20.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.7974432312237534, total=  14.1s\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=120, score=-0.7935780525064576, total=  13.6s\n",
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=-0.7644887336956103, total=  25.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  21 out of  24 | elapsed:  1.2min remaining:    9.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  lr__C=10, lr__penalty=l1, svd__n_components=180, score=-0.7617696744562086, total=  25.6s\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.7603313246999102, total=  16.6s\n",
      "[CV]  lr__C=10, lr__penalty=l2, svd__n_components=180, score=-0.7570229659599857, total=  16.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.757\n",
      "Best parameters set:\n",
      "\tlr__C: 0.1\n",
      "\tlr__penalty: 'l2'\n",
      "\tsvd__n_components: 180\n"
     ]
    }
   ],
   "source": [
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] nb__alpha=0.001 .................................................\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] nb__alpha=0.01 ..................................................\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] nb__alpha=0.1 ...................................................\n",
      "[CV] ....... nb__alpha=0.001, score=-0.6925622523613629, total=   0.0s\n",
      "[CV] ....... nb__alpha=0.001, score=-0.6696216900119139, total=   0.0s\n",
      "[CV] ........ nb__alpha=0.01, score=-0.5610530967007644, total=   0.0s\n",
      "[CV] ........ nb__alpha=0.01, score=-0.5517055966352052, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] ......... nb__alpha=0.1, score=-0.5263054904291952, total=   0.0s\n",
      "[CV] nb__alpha=1 .....................................................\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] nb__alpha=10 ....................................................\n",
      "[CV] ......... nb__alpha=0.1, score=-0.5269509257179364, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] ........... nb__alpha=1, score=-0.7022225786462692, total=   0.0s\n",
      "[CV] nb__alpha=100 ...................................................\n",
      "[CV] .......... nb__alpha=10, score=-0.9727345547502457, total=   0.0s\n",
      "[CV] ........... nb__alpha=1, score=-0.7020455303894876, total=   0.0s\n",
      "[CV] .......... nb__alpha=10, score=-0.9725725105286791, total=   0.0s\n",
      "[CV] ......... nb__alpha=100, score=-1.0719561147051138, total=   0.0s\n",
      "[CV] ......... nb__alpha=100, score=-1.0720054570650377, total=   0.0s\n",
      "Best score: -0.527\n",
      "Best parameters set:\n",
      "\tnb__alpha: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.0597s.) Setting batch_size=6.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  12 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Create the pipeline \n",
    "clf = pipeline.Pipeline([('nb', nb_model)])\n",
    "\n",
    "# parameter grid\n",
    "param_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n",
    "                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(xtrain_tfv, ytrain)  # we can use the full data here but im only using xtrain. \n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Glove Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
